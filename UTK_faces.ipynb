{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10d3e306",
   "metadata": {},
   "source": [
    "## OPIS: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684f2688",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## 1. Pobieranie danych\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5dac8951",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f33109fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "age_classes = 6\n",
    "\n",
    "def class_labels_reassign(age):\n",
    "    if 1 <= age <= 2:\n",
    "        return 0\n",
    "    elif 3 <= age <= 9:\n",
    "        return 1\n",
    "    elif 10 <= age <= 20:\n",
    "        return 2\n",
    "    elif 21 <= age <= 27:\n",
    "        return 3\n",
    "    elif 28 <= age <= 45:\n",
    "        return 4\n",
    "    elif 46 <= age <= 65:\n",
    "        return 5\n",
    "    else:\n",
    "        return 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24bb8db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dane wkładane są do listy z {age_classes} wierszami. \n",
    "# W każym wierszu znajduje się zdjęcie/wiek odpowiadający danej grupie wiekowej.\n",
    "\n",
    "def load_data(data_folder, img_width, img_height, N):\n",
    "    images = []\n",
    "    ages = []\n",
    "    \n",
    "    for _ in range(age_classes):\n",
    "        new_list1 = []  \n",
    "        new_list2 = []\n",
    "        images.append(new_list1) \n",
    "        ages.append(new_list2)\n",
    "        \n",
    "    i = 0\n",
    "    for filename in os.listdir(data_folder):\n",
    "            img = cv2.imread(os.path.join(data_folder, filename))\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) \n",
    "            img = cv2.resize(img, (img_width, img_height))  \n",
    "            age = int(filename.split('_')[0])  \n",
    "            age_class = class_labels_reassign(age) - 1\n",
    "            \n",
    "            ages[age_class].append(age)\n",
    "            images[age_class].append(img)\n",
    "            \n",
    "            if(i==N and N!=-1):\n",
    "                break\n",
    "            i = i + 1\n",
    "            \n",
    "    return images, ages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01a49002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parametry\n",
    "img_width = 200\n",
    "img_height = 200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3393c474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pobierz dane\n",
    "\n",
    "(img1, ages1) = load_data('./dataset/UTKface dataset/part1', img_width, img_height, 100)\n",
    "(img2, ages2) = load_data('./dataset/UTKface dataset/part2', img_width, img_height, 100)\n",
    "(img3, ages3) = load_data('./dataset/UTKface dataset/part3', img_width, img_height, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ba238ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Połącz listy, łacząc wszystkie grupy wiekowe\n",
    "\n",
    "def concatenate_lists_age_class(*lists):\n",
    "    output_lists = []\n",
    "    \n",
    "    for _ in range(age_classes):\n",
    "        new_list = []  # Tworzymy nową pustą listę\n",
    "        output_lists.append(new_list) \n",
    "    \n",
    "    for list_ in lists:\n",
    "        i = 0\n",
    "        for l in list_:\n",
    "            output_lists[i].extend(l)\n",
    "            i = i + 1\n",
    "    return output_lists\n",
    "\n",
    "images = concatenate_lists_age_class(img1,img2,img3)\n",
    "ages = concatenate_lists_age_class(ages1,ages2,ages3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb4e9eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3c2d9399",
   "metadata": {},
   "source": [
    "## 2. Augmentacja danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fbf3887",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "928ac9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_image(image, N=5):\n",
    "    \n",
    "    # Wczytaj obraz za pomocą Keras\n",
    "    image_array = image\n",
    "    image_array = np.expand_dims(image_array, axis=0)\n",
    "\n",
    "    # Konfiguracja generatora augmentacji\n",
    "    generator = ImageDataGenerator(\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest'\n",
    "    )\n",
    "    \n",
    "    # Wygeneruj N zaaugmentowanych obrazów\n",
    "    augmented_images = []\n",
    "    augmented_images.append(np.array(image).astype(float))\n",
    "    for _ in range(N):\n",
    "        augmented_image = generator.flow(image_array).next()[0]\n",
    "        augmented_images.append(augmented_image)\n",
    "\n",
    "    return augmented_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "952fc00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_list(images, ages, N=5):\n",
    "    images_output = []\n",
    "    ages_output = []\n",
    "    \n",
    "    for i, (image, age) in enumerate(zip(images, ages)):\n",
    "\n",
    "        augmented_images = augment_image(image, N)\n",
    "\n",
    "        augmented_ages = [age] * (N + 1)\n",
    "        \n",
    "        images_output.extend(augmented_images)\n",
    "        ages_output.extend(augmented_ages)\n",
    "        \n",
    "    return (images_output, ages_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab38fd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wektor ile zdjęć augmentować dla danej grupy wiekowe\n",
    "# Na przykład: jeżeli i-ty element wektora równa się 5 to funkcja stworzy 5 przekształconych zdjęć + 1 zdjęcie oryginalne\n",
    "augment_amount_for_class = [5, 5, 5, 7, 8, 15]\n",
    "\n",
    "for i, N in enumerate(augment_amount_for_class):\n",
    "    images[i], ages[i] = augment_list(images[i], ages[i], N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba3d9512",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQEAAAEICAYAAABf40E1AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAAAUoElEQVR4nO3de7RcZX3G8e9DuCkXE0iMGC7hEqigNuAp4AXEhRdgWQNeMFkVQalAF7RUu2hB11LsqharmKpVKBRKUK41pmTVqEQq4VIQEk0RUCSERBJCAuEqUCDh1z/2O2ZnMsM5mT1z9pzzPp+1zjp73n2Z355knnn35cyriMDM8rVF3QWYWb0cAmaZcwiYZc4hYJY5h4BZ5hwCZplzCPQhSadLmiNpy81c7x5JR3T4nCdJuqWTdVts6/eS9urGttpsPyTt06vt58YhUIGkcyT9qKnt/jZt04e4zXcD7wU+GhHrNqeeiDggIm4cwnNMTm+kzQqZzahj+4hY2sm6knaVdIWktZKelXSHpPd3u0bbwCFQzU3A2ySNAZC0C7AVcGBT2z5p2UFFxE8jYlpEvNijmvuWpJ2AW4AXgQOA8cBM4EpJH+5wmz0JutHEIVDNnRRv+qnp8WHAz4D7mtoeiIiHJd0t6U8bK0vaStJjkg6UtK2k76VPwCcl3SlpoqR3SfpVaZ35ku4sPb5Z0rFpelnqSSBpC0lnS3ogbfPa9CaDDYH0ZOq6v7V5xyT9UXquxyXdJ+n40rzLJH1b0g8lPSPp55L2Ls3/Q3dd0jaSvibpd5JWS7pQ0qvavJ6fBn4PnBwRj0TE8xFxFfAl4HxJKi17jKSl6fX7qqQt0vOdJOlWSTMlrQXOlbS3pP9Or8NjqacxNi3/0fQaNH5ekHRjmnejpD8v7VfXDpn6iUOggvRp/XPg8NR0OHAzxadZua3xprsc+FhpE8cAqyLil8CJwGuA3YCdgdOA54HbgSmSxkvaCngz8HpJO6Q300B6zmZ/CRwLvBN4PfAE8O1STQBjU9f9tvKKkrYD5gNXAq8FpgPfkbR/abHpwBeBccASijdqK+cB+1KE4j7AJODzbZZ9DzA7Il5uar8W2D1tp+E4in0/CJgGfLI07xBgKTAx1SXgHylehzdQvMbnAkTENek12D7NXwpc1aa+UckhUN0CNrypDqN4Q97c1LYgTX+P4hNsx/T4BOC7afolijf/PhGxPiIWRcTTEfE8RY/jcOAtwP8CtwJvBw4F7o+ItS3qOg34XESsiIgXKP7Tf3iI3eP3A8si4t8jYl0KqdnAR0rLzImIO9J5iyvY0PP5g/TJfQrw6Yh4PCKeAb5MESCtjAdWtWhfVZrf8JW0zd8B/wzMKM17OCK+lWp/PiKWRMT8iHghIh4Fvk4RjuVat6AIvRsj4l/b1Dcq+XipupuA01NXe0JE3C9pNTArtb0xLUM6JLgV+JCkOcDRwJlpO9+l+IS6OnVVv0fxJn6JIkSOAFak6Sco/hO/wIaAabYHMEdS+VN1PcWn42D2AA6R9GSpbUs2BBbAI6Xp54DtW2xnAvBqYFGpJy9gTJvnfQzYpUX7LqX5DQ+VppdTfIq3moekicA3KAJ5B4oPvyeanuNLad5ftalt1HJPoLrbKLrxn6L4hCYingYeTm0PR8SDpeVnURwSfAS4LSJWpnVeiogvRsT+wNsoPo0/ntZphMDhaXoBRQi8k/Yh8BBwdESMLf1sm55vsD8dfQhY0LTu9hHxF0N8TRoeozikOaC0ndekrncrPwU+2Di+Lzk+1fTbUttupendKV7vhub9+3Jqe1NE7Ejx+m9IpeLKzQzgwyl0G56lCLGG17Wpe0RzCFSUuusLgc+w8bH5Lamt+arAf1Icx55JcY4AgHQC8E3pqsLTFIcHjU/x/wH2Aw4G7oiIe0if1i2233Ah8CVJe6TtT5A0Lc17NG273bX8/wL2lXRCOnm5laQ/kfSG9q/EptKx/cXATEmvTXVMkvS+NqvMpAjUSyS9Lp0snQF8DjgrNv6797MkjZO0G8Vrec0rlLIDxQnHpyRNAs5qzJB0IPAt4Nh0qFC2mCKUXp1OdJ48tD0fWRwC3bGA4gRa+czxzaltozdpCo3ZwJ7AD0qzXgd8nyIAfp22+d20zrPAL4B7SpcObwOWR8SaNjV9A5gLXC/pGYoTjIek7T1H0f29NV2JOLSpxmco7lWYTvEJ+wjwFWCbIbwWzf6O4sTh7ZKepvi036/VguncxjuAbYF7gbUUQXpCRDS/ya8DFlG8UX8IXPIKNXyRInifSsuWX/dpFCc3byldIWjc5zGT4nLlaooe3BVD2N8RR/5SkeEn6fPAvhHxsUEXNusxnxgcZulk4ckUVwbMaufDgWEk6VMUJ7h+FBFDuoPQrNd6djgg6SiK49IxwL9FxHk9eSIzq6QnIZDOcP+W4g6wFRQ3u8yIiHu7/mRmVkmvzgkcDCxp/CWZpKspzsK2DIHx48fH5MmTe1SKmQEsWrTosYiY0NzeqxCYxMZ3ba0gXZ5qkHQKxS2l7L777ixcuLBHpZgZgKTlrdprOzEYERdFxEBEDEyYsEk4mdkw6VUIrGTj2zp3TW1m1md6FQJ3Uvz5656Stqa482xuj57LzCro1ddLrZN0BvATikuEl6b73c2sz/TsjsGImAfM69X2zaw7fMegWeYcAmaZcwiYZc4hYJY5h4BZ5hwCZplzCJhlziFgljmHgFnmHAJmmXMImGXOIWCWOYeAWeYcAmaZcwiYZc4hYJY5h4BZ5hwCZpnrOAQk7SbpZ5LulXSPpDNT+7mSVkpanH6O6V65dbiZYoRss9GpyncMrgP+JiJ+IWkHYJGk+WnezIj4WvXy+sEhePBmG806/t8dEauAVWn6GUm/phh5aJTZuu4CzHqqK+cEJE0GDgR+nprOkHSXpEsljWuzzimSFkpa+Oijj3ajDDPrQOUQkLQ9MBv464h4GrgA2BuYStFTOL/Veh6GzKw/VAoBSVtRBMAVEfEDgIhYHRHrI+Jl4GKKEYrNrE9VuTog4BLg1xHx9VL7LqXFjgPu7rw8M+u1Kqe93w6cAPxK0uLU9llghqSpQADLgFMrPIeZ9ViVqwO3AGoxy0OPmY0gvmPQLHMOAbPMOQTMMucQMMucQ8D6x5PAP9RdRH4cAtY/dsAXlGvgELD+MQbwHeTDziFgljmHgFnmHAJmmXMImGXOIWCWOYeAWeYcAmaZcwiYZc4hYJY5h4BZ5hwCZpmrPLSOpGXAM8B6YF1EDEjaCbgGmEzxPYPHR8QTVZ/LzLqvWz2Bd0XE1IgYSI/PBm6IiCnADemxmfWhXh0OTANmpelZwLE9eh4zq6gbIRDA9ZIWSToltU1MYxUCPAJMbF7Jw5CZ9YduDLf7johYKem1wHxJvynPjIiQFM0rRcRFwEUAAwMDm8w3s+FRuScQESvT7zXAHIphx1Y3RiJKv9dUfR4z642qYxFuJ2mHxjTwXophx+YCJ6bFTgSuq/I8ZtY7VQ8HJgJzimEJ2RK4MiJ+LOlO4FpJJwPLgeMrPo+Z9UilEIiIpcAft2hfCxxZZdtmNjx8x6BZ5hwCZplzCJhlziFgljmHgFnmHAJmmXMImGXOIWCWOYeAWeYcAmaZcwiYZc4hYJY5h4BZ5hwCZplzCJhlziFgljmHgFnmHAJmmev468Uk7Ucx1FjDXsDngbHAp4DGYAKfjYh5nT6PmfVWxyEQEfcBUwEkjQFWUnzl+CeAmRHxtW4UaGa91a3DgSOBByJieZe2Z2bDpFshMB24qvT4DEl3SbpU0rhWK3gYMrP+UDkEJG0NfAD4j9R0AbA3xaHCKuD8VutFxEURMRARAxMmTKhahpl1qBs9gaOBX0TEaoCIWB0R6yPiZeBiimHJzKxPdSMEZlA6FGiMQZgcRzEsmZn1qUojEKXxB98DnFpq/idJUymGLF/WNM/M+kzVYcieBXZuajuhUkVmNqx8x6BZ5hwCZplzCJhlziFgljmHgFnmHAJmmXMImGXOIWCWOYeAWeYcAmaZcwiYZc4hYJY5h4BZ5hwCZplzCJhlziFgljmHgFnmHAJmmRtSCKTxA9ZIurvUtpOk+ZLuT7/HpXZJ+qakJWnsgYN6VbyZVTfUnsBlwFFNbWcDN0TEFOCG9BiKryCfkn5OoRiHwMz61JBCICJuAh5vap4GzErTs4BjS+2XR+F2YGzT15CbWR+pck5gYkSsStOPABPT9CTgodJyK1KbmfWhrpwYjIigGGdgyDwWoVl/qBICqxvd/PR7TWpfCexWWm7X1LYRj0Vo1h+qhMBc4MQ0fSJwXan94+kqwaHAU6XDBjPrM0MagUjSVcARwHhJK4AvAOcB10o6GVgOHJ8WnwccAywBngM+0eWazayLhhQCETGjzawjWywbwOlVijKz4eM7Bs0y5xAwy5xDwCxzDgGzzDkEzDLnEDDLnEPALHMOAbPMOQTMMucQMMucQ8Ascw4Bs8w5BMwy5xAwy5xDwCxzDgGzzDkEzDLnEDDLnEPALHODhkCbcQi/Kuk3aazBOZLGpvbJkp6XtDj9XNjD2s2sC4bSE7iMTcchnA+8MSLeDPwWOKc074GImJp+TutOmWbWK4OGQKtxCCPi+ohYlx7eTjHAiJmNQN04J/BJ4Eelx3tK+qWkBZIOa7eShyEz6w+VQkDS54B1wBWpaRWwe0QcCHwGuFLSjq3W9TBkZv2h4xCQdBLwfuDP0oAjRMQLEbE2TS8CHgD27UKdZtYjHYWApKOAvwU+EBHPldonSBqTpvcCpgBLu1GomfXGoMOQtRmH8BxgG2C+JIDb05WAw4G/l/QS8DJwWkQ83nLDZtYXBg2BNuMQXtJm2dnA7KpFmdnw8R2DZplzCJhlziFgljmHgFnmHAJmmXMImGXOIWCWOYeA2Wb6OrC87iK6yCFgtpkmAFvXXUQXDXrHoJlt7IS6C+gy9wTMMucQMMucQ8Ascw4Bswoi/YxkDgGzCtYCa+ouoiJfHTCrYOe6C+gC9wTMKhDF9/EvqbuQChwCZhWNBbYCrqu5jk51OgzZuZJWloYbO6Y07xxJSyTdJ+l9vSrcrF+MASYCh9ZdSIc6HYYMYGZpuLF5AJL2B6YDB6R1vtP49mGz0Wxb4FXAvLoL6UBHw5C9gmnA1Wn8gQcpDpUOrlCf2YgxhuLQYKSpck7gjDQq8aWSxqW2ScBDpWVWpLZNeBgyG222ozgkeKLuQjZTpyFwAbA3MJVi6LHzN3cDHobMRqMAnqq7iM3UUQhExOqIWB8RLwMXs6HLvxLYrbTorqnNLAtjgD2Alxg5dxJ2OgzZLqWHxwGNKwdzgemStpG0J8UwZHdUK9Fs5BlJB7idDkN2hKSpFGG3DDgVICLukXQtcC/FaMWnR8T6nlRu1qcEvL7uIjaD0oDCtRoYGIiFCxfWXYZZVwVFIPQLSYsiYqC53XcMmvVAADsBz9RdyBA4BMx65D6Ky4b9ziFg1iPj6a/DgXb8p8RmPSBGRgCAewJm2XMImGXOIWDWQ3OBZ+suYhAOAbMe+iHwXN1FDMIhYNZDXwDGDbpUvXx1wKyHRsLtw+4JmGXOIWCWOYeAWeYcAmY9diZwf91FvAKHgFmPfQjo5y/Q89UBsx57C8VXkvcrh4BZj/X7nxP7cMAsc50OQ3ZNaQiyZZIWp/bJkp4vzbuwh7WbWRcM5XDgMuBfgMsbDRHx0ca0pPPZ+KvWH4iIqV2qz2xUWE5x+/COdRfSQqVhyCQJOB64qst1mY0q1wO/q7uINqqeEzgMWB0R5cuge0r6paQFkg5rt6KHIbOcvBHYue4i2qgaAjPYuBewCtg9Ig4EPgNcKallD8jDkFlO3grsMuhS9eg4BCRtCXwQuKbRlkYjXpumFwEPAPtWLdLMeqdKT+DdwG8iYkWjQdIESWPS9F4Uw5AtrVaimfXSUC4RXgXcBuwnaYWkk9Os6Wx6QvBw4K50yfD7wGkR0fKkopn1h0EvEUbEjDbtJ7Vomw3Mrl6W2ehzM8WoRAfUXUgT3zFoNkz+D3ix7iJa8N8OmA2TNwPb1F1ECw4Bs2Eyse4C2vDhgFnmHAJmmXMImA2jFyhOEPYTh4DZMHqQ/vu+QYeA2TBaCTxUdxFNfHXAbBgdWXcBLbgnYJY5h4BZ5hwCZplzCJgNs2eBx+ouosQhYDbMlgAL6i6ixFcHzIbZeCDqLqLEIWA2zCaln37hwwGzzDkEzDLnEDDLnEPArAYv0z9fNeYQMKvBYuCsuotIFFH/xQpJj9J/91B0y3hG537B6N230bpfe0TEJsN99UUIAEhaGBEDddfRbaN1v2D07tto3a92fDhgljmHgFnm+ikELqq7gB4ZrfsFo3ffRut+tdQ35wTMrB791BMwsxo4BMwyV3sISDpK0n2Slkg6u+56qpK0TNKvJC2WtDC17SRpvqT70+9xddc5GEmXSloj6e5SW8v9UOGb6d/wLkkH1Vf54Nrs27mSVqZ/t8WSjinNOyft232S3ldP1b1TawhIGgN8Gzga2B+YIWn/OmvqkndFxNTSteazgRsiYgpwQ3rc7y4Djmpqa7cfRwNT0s8pwAXDVGOnLmPTfQOYmf7dpkbEPID0/3E6xYjiRwHfSf9vR426ewIHA0siYmlEvAhcDUyruaZemAbMStOzgGPrK2VoIuIm4PGm5nb7MQ24PAq3A2Ml7TIshXagzb61Mw24OiJeiIgHKb4Y6OCeFVeDukNgEhuPxbCC/vq+hU4EcL2kRZJOSW0TI2JVmn6E/h2gdjDt9mO0/DuekQ5nLi0dso2WfWur7hAYjd4REQdRdJFPl3R4eWYU12RH/HXZ0bIfJRcAewNTgVXA+bVWM4zqDoGVwG6lx7umthErIlam32uAORRdx9WN7nH6vaa+Citptx8j/t8xIlZHxPqIeBm4mA1d/hG/b4OpOwTuBKZI2lPS1hQnYObWXFPHJG0naYfGNPBe4G6KfToxLXYicF09FVbWbj/mAh9PVwkOBZ4qHTaMCE3nMI6j+HeDYt+mS9pG0p4UJz/vGO76eqnWLxqNiHWSzgB+AowBLo2Ie+qsqaKJwBxJULy2V0bEjyXdCVwr6WRgOXB8jTUOiaSrgCOA8ZJWAF8AzqP1fswDjqE4afYc8IlhL3gztNm3IyRNpTjEWQacChAR90i6FrgXWAecHhHrayi7Z3zbsFnm6j4cMLOaOQTMMucQMMucQ8Ascw4Bs8w5BMwy5xAwy9z/A9U7RArwhoHLAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "images[1][1]\n",
    "plt.imshow(augment_image(images[1][10])[3],vmin=0.0, vmax=1.0)# cmap='gray' dla obrazu czarno-białego vmin=0.0, vmax=1.0\n",
    "plt.title('Wyświetlenie Obrazu')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98aa0ce9",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Zbiory treningowe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "524965c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17908ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Łączenie wiele list w jedną listę.\n",
    "def concatenate_lists(*lists):\n",
    "    output_lists = []\n",
    "\n",
    "    for list_ in lists:\n",
    "        new_list.extend(list_)\n",
    "        \n",
    "    return output_lists\n",
    "\n",
    "# Bierze listę zdjęć/wieków podzieloną na klasy wiekowe i łaczy w jedną listę.\n",
    "def merge_lists(lists):\n",
    "    output_lists = []\n",
    "    i = 0\n",
    "    for _ in range(age_classes):\n",
    "        output_lists.extend(lists[i]) \n",
    "        i = i + 1\n",
    "    \n",
    "    return output_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b69242ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przygotowanie zbiorów testowych/treningowych.\n",
    "seed = 42\n",
    "\n",
    "images = merge_lists(images)\n",
    "ages = merge_lists(ages)\n",
    "\n",
    "images = np.array(images)\n",
    "ages = np.array(ages)\n",
    "\n",
    "# Podziel dane na dane treningowe (80%) i dane testowe (20%)\n",
    "train_images, test_images, train_ages, test_ages = train_test_split(images, ages, test_size=0.2, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5ee16fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funkcja do przetwarzania obrazów\n",
    "# Tutaj możemy na przykład przerobić obraz na czarno-biały.\n",
    "\n",
    "def preprocess_image(image, age):\n",
    "    image = image / 255.0  # Normalizacja wartości pikseli do zakresu [0, 1]\n",
    "    \n",
    "    return image, age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "60e5eaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_param = 512\n",
    "\n",
    "# Utwórz dataset dla danych treningowych\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_ages))\n",
    "train_dataset = train_dataset.map(preprocess_image)\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "\n",
    "# Utwórz dataset dla danych testowych\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_ages))\n",
    "test_dataset = test_dataset.map(preprocess_image)\n",
    "test_dataset = test_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f67960a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a5ec0fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liczba obrazów treningowych: 1710\n",
      "Liczba wieków treningowych: 1710\n",
      "Liczba obrazów walidacyjnych: 428\n",
      "Liczba wieków walidacyjnych: 428\n"
     ]
    }
   ],
   "source": [
    "# Wyświetl rozmiary danych treningowych i walidacyjnych\n",
    "print(f\"Liczba obrazów treningowych: {len(train_images)}\")\n",
    "print(f\"Liczba wieków treningowych: {len(train_ages)}\")\n",
    "print(f\"Liczba obrazów walidacyjnych: {len(test_images)}\")\n",
    "print(f\"Liczba wieków walidacyjnych: {len(test_ages)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f90a58",
   "metadata": {},
   "source": [
    "# 4. Tworzenie modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "38d973cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Conv2D, AveragePooling2D, GlobalAveragePooling2D\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9fa78089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 198, 198, 32)      896       \n",
      "                                                                 \n",
      " average_pooling2d (AverageP  (None, 99, 99, 32)       0         \n",
      " ooling2D)                                                       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 97, 97, 64)        18496     \n",
      "                                                                 \n",
      " average_pooling2d_1 (Averag  (None, 48, 48, 64)       0         \n",
      " ePooling2D)                                                     \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 46, 46, 128)       73856     \n",
      "                                                                 \n",
      " average_pooling2d_2 (Averag  (None, 23, 23, 128)      0         \n",
      " ePooling2D)                                                     \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 21, 21, 256)       295168    \n",
      "                                                                 \n",
      " average_pooling2d_3 (Averag  (None, 10, 10, 256)      0         \n",
      " ePooling2D)                                                     \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 256)              0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 132)               33924     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 7)                 931       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 8         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 423,279\n",
      "Trainable params: 423,279\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Input layer with 32 filters, followed by an AveragePooling2D layer.\n",
    "model.add(Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=(img_width, img_height, 3)))    # 3rd dim = 1 for grayscale images.\n",
    "model.add(AveragePooling2D(pool_size=(2,2)))\n",
    "\n",
    "# Three Conv2D layers with filters increasing by a factor of 2 for every successive Conv2D layer.\n",
    "model.add(Conv2D(filters=64, kernel_size=3, activation='relu'))\n",
    "model.add(AveragePooling2D(pool_size=(2,2)))\n",
    "model.add(Conv2D(filters=128, kernel_size=3, activation='relu'))\n",
    "model.add(AveragePooling2D(pool_size=(2,2)))\n",
    "model.add(Conv2D(filters=256, kernel_size=3, activation='relu'))\n",
    "model.add(AveragePooling2D(pool_size=(2,2)))\n",
    "\n",
    "# A GlobalAveragePooling2D layer before going into Dense layers below.\n",
    "# GlobalAveragePooling2D layer gives no. of outputs equal to no. of filters in last Conv2D layer above (256).\n",
    "model.add(GlobalAveragePooling2D())\n",
    "# One Dense layer with 132 nodes so as to taper down the no. of nodes from no. of outputs of GlobalAveragePooling2D layer above towards no. of nodes in output layer below (7).\n",
    "model.add(Dense(132, activation='relu'))\n",
    "# Output layer with 7 nodes (equal to the no. of classes).\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "model.add(Dense(1, activation='linear'))  # Warstwa wyjściowa z jednym neuronem (regresja)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Compiling the above created CNN architecture.\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6fee3539",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path  = './checkpoints/UTK_faces/age_model_checkpoint.h5'\n",
    "checkpoint = ModelCheckpoint(filepath=checkpoint_path,\n",
    "                             monitor='val_accuracy',\n",
    "                             save_best_only=True,\n",
    "                             save_weights_only=False,\n",
    "                             verbose=1\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42bcc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trenowanie modelu\n",
    "\n",
    "model_history = model.fit(train_dataset, epochs=10, batch_size=batch_size_param, shuffle=True, validation_data=(test_images, test_ages))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910d38e8",
   "metadata": {},
   "source": [
    "# 5. Testowanie Modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93def57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Historia trenowania\n",
    "\n",
    "train_loss = model_history.history['loss']\n",
    "test_loss = model_history.history['val_loss']\n",
    "train_accuracy = model_history.history['accuracy']\n",
    "test_accuracy = model_history.history['val_accuracy']\n",
    "\n",
    "# Plotting a line chart to visualize the loss and accuracy values by epochs.\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(15,7))\n",
    "ax = ax.ravel()\n",
    "ax[0].plot(train_loss, label='Train Loss', color='royalblue', marker='o', markersize=5)\n",
    "ax[0].plot(test_loss, label='Test Loss', color = 'orangered', marker='o', markersize=5)\n",
    "ax[0].set_xlabel('Epochs', fontsize=14)\n",
    "ax[0].set_ylabel('Categorical Crossentropy', fontsize=14)\n",
    "ax[0].legend(fontsize=14)\n",
    "ax[0].tick_params(axis='both', labelsize=12)\n",
    "ax[1].plot(train_accuracy, label='Train Accuracy', color='royalblue', marker='o', markersize=5)\n",
    "ax[1].plot(test_accuracy, label='Test Accuracy', color='orangered', marker='o', markersize=5)\n",
    "ax[1].set_xlabel('Epochs', fontsize=14)\n",
    "ax[1].set_ylabel('Accuracy', fontsize=14)\n",
    "ax[1].legend(fontsize=14)\n",
    "ax[1].tick_params(axis='both', labelsize=12)\n",
    "fig.suptitle(x=0.5, y=0.92, t=\"Lineplots showing loss and accuracy of CNN model by epochs\", fontsize=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62162586",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_score = model.evaluate(test_dataset, verbose=1)\n",
    "# Printing the relevant score summary.\n",
    "final_labels = model.metrics_names\n",
    "print(f'CNN model {final_labels[0]} \\t\\t= {round(final_score[0], 3)}')\n",
    "print(f'CNN model {final_labels[1]} \\t= {round(final_score[1], 3)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a06838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model as a h5 file for possible use later.\n",
    "saved_model_path = './models_saved/UTK_faces.h5'\n",
    "model.save(saved_model_path, save_format='h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e998bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating predictions from the model above.\n",
    "final_pred = model.predict(test_dataset)\n",
    "final_pred = final_pred.argmax(axis=-1)\n",
    "\n",
    "# Generating a confusion matrix based on above predictions.\n",
    "conf_mat = confusion_matrix(test_ages, final_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db84c425",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion Matrix', export_as='confusion_matrix', cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "    # print(cm)\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True labels', fontsize=14)\n",
    "    plt.xlabel('Predicted labels', fontsize=14)\n",
    "    \n",
    "    # Exporting plot image in PNG format.\n",
    "    #plt.savefig(f'/content/drive/My Drive/Project5_AgeGenderEmotion_Detection/1.1_age_input_output/output/cnn_logs/{export_as}.png', bbox_inches='tight');\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059745bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_plot_labels = ['1-2', '3-9', '10-20', '21-27', '28-45', '46-65', '66-116']\n",
    "\n",
    "plt.figure(figsize=(16,8))\n",
    "\n",
    "plot_confusion_matrix(conf_mat, cm_plot_labels, normalize=True,\n",
    "                      title=\"Confusion Matrix based on predictions from CNN model\",\n",
    "                      export_as=\"final_cnn_conf_mat_norm\"\n",
    "                     )\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
